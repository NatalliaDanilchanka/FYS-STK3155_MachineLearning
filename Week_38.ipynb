{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da77599",
   "metadata": {},
   "source": [
    "# Exercises week 38\n",
    "## September 15-19\n",
    "\n",
    "## Resampling and the Bias-Variance Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f27b0e",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "After completing these exercises, you will know how to\n",
    "- Derive expectation and variances values related to linear regression\n",
    "- Compute expectation and variances values related to linear regression\n",
    "- Compute and evaluate the trade-off between bias and variance of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984af8e3",
   "metadata": {},
   "source": [
    "This week deals with various mean values and variances in linear regression methods (here it may be useful to look up chapter 3, equation (3.8) of [Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, The Elements of Statistical Learning, Springer](https://www.springer.com/gp/book/9780387848570)). The exercises are also a part of project 1 and can be reused in the theory part of the project.\n",
    "\n",
    "For more discussions on Ridge regression and calculation of expectation values, [Wessel van Wieringen's](https://arxiv.org/abs/1509.09169) article is highly recommended.\n",
    "\n",
    "We assume that there exists a continuous function $f(\\boldsymbol{x})$ and a normal distributed error $\\boldsymbol{\\varepsilon}\\sim N(0, \\sigma^2)$ which describes our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f7d0e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{y} = f(\\boldsymbol{x})+\\boldsymbol{\\varepsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcf981a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We further assume that this continous function can be modeled with a linear model $\\mathbf{\\tilde{y}}$ of some features $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4189366",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{y} = \\boldsymbol{\\tilde{y}} + \\boldsymbol{\\varepsilon} = \\boldsymbol{X}\\boldsymbol{\\beta} +\\boldsymbol{\\varepsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fca21b",
   "metadata": {},
   "source": [
    "We therefore get that our data $\\boldsymbol{y}$ has an expectation value $\\boldsymbol{X}\\boldsymbol{\\beta}$ and variance $\\sigma^2$, that is $\\boldsymbol{y}$ follows a normal distribution with mean value $\\boldsymbol{X}\\boldsymbol{\\beta}$ and variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0c7e6",
   "metadata": {},
   "source": [
    "## Exercise 1: Expectation values for ordinary least squares expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878c699",
   "metadata": {},
   "source": [
    "**a)** With the expressions for the optimal parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ show that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7007d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\hat{\\beta}_{OLS}}) = \\boldsymbol{\\beta}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3878d-2390-45a2-9dee-7e3cfb2602ca",
   "metadata": {},
   "source": [
    "With $ \\boldsymbol{\\beta} = (\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\mathbf{X}^{T} \\mathbf{Y} $ we can compute expectation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7c2b5-699e-4a68-a4e4-3cacc9c5fe70",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(\\boldsymbol{\\beta}) = \\mathbb{E}[ (\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\mathbf{X}^{T} \\mathbf{Y}]=(\\mathbf{X}^{T} \\mathbf{X})^{-1}\\mathbf{X}^{T} \\mathbb{E}[ \\mathbf{Y}]=(\\mathbf{X}^{T} \\mathbf{X})^{-1} \\mathbf{X}^{T}\\mathbf{X}\\boldsymbol{\\beta}=\\boldsymbol{\\beta}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e93394",
   "metadata": {},
   "source": [
    "**b)** Show that the variance of $\\boldsymbol{\\hat{\\beta}_{OLS}}$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b65be",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Var}(\\boldsymbol{\\hat{\\beta}_{OLS}}) = \\sigma^2 \\, (\\mathbf{X}^{T} \\mathbf{X})^{-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69bcbf-1435-4bf8-94fe-ffa95b52fa35",
   "metadata": {},
   "source": [
    "For vaiance we have following expression: $\\mathbf{Var}(A * \\boldsymbol{\\theta})= A * \\mathbf{Var}(\\boldsymbol{\\theta}) A^T $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22548503-900d-4e38-960a-a34648729a9f",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\mathbf{Var}(\\boldsymbol{\\theta})= \\mathbf{Var}( (\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\mathbf{X}^{T} \\mathbf{Y}) =\n",
    "$$\n",
    "$$\n",
    "    ((\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\mathbf{X}^{T}) \\mathbf{Var}(\\mathbf{Y})((\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\mathbf{X}^{T})^T=\n",
    "    ((\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\mathbf{X}^{T}) \\sigma^2 ((\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\mathbf{X}^{T})^T = \n",
    "$$\n",
    "$$\n",
    "    \\sigma^2 ((\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\mathbf{X}^{T})((\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\mathbf{X}^{T})^T =\n",
    "    \\sigma^2 (\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{X}((\\mathbf{X}^{\\top} \\mathbf{X})^{-1})^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a82f76-80a8-4077-b67d-449ec022fc31",
   "metadata": {},
   "source": [
    "From basic matrix operation, we know that $A^{-1}A = \\mathbb{1}$. Thus:\n",
    "$$\n",
    "(\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{X}= \\mathbb{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe02ce8-35b6-4f00-9242-1d96cd92accc",
   "metadata": {},
   "source": [
    "$$\n",
    "  \\mathbf{Var}(\\boldsymbol{\\theta}) =\n",
    "  \\sigma^2 \\mathbb{1}((\\mathbf{X}^{\\top} \\mathbf{X})^{-1})^T =\n",
    "  \\sigma^2 \\mathbb{1}((\\mathbf{X}^{\\top} \\mathbf{X})^{T})^{-1} =\n",
    "  \\sigma^2 \\mathbb{1}(\\mathbf{X}^{\\top} \\mathbf{X})^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2dc22",
   "metadata": {},
   "source": [
    "## Exercise 2: Expectation values for Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893e3e7",
   "metadata": {},
   "source": [
    "**a)** With the expressions for the optimal parameters $\\boldsymbol{\\hat{\\beta}_{Ridge}}$ show that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc571f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E} \\big[ \\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}} \\big]=(\\mathbf{X}^{T} \\mathbf{X} + \\lambda \\mathbf{I}_{pp})^{-1} (\\mathbf{X}^{\\top} \\mathbf{X})\\boldsymbol{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028209a1",
   "metadata": {},
   "source": [
    "We see that $\\mathbb{E} \\big[ \\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}} \\big] \\not= \\mathbb{E} \\big[\\hat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}\\big ]$ for any $\\lambda > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad260b2-aac1-4129-81d6-861ee930449c",
   "metadata": {},
   "source": [
    "With $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{Ridge}} = (\\boldsymbol{X}^T\\boldsymbol{X} + \\boldsymbol{\\lambda}\\boldsymbol{I})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db3140-676e-41fb-a557-cc4cd7192ed8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E} \\big[ \\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}} \\big]=\n",
    "\\mathbb{E} \\big[ (\\boldsymbol{X}^T\\boldsymbol{X} + \\boldsymbol{\\lambda}\\boldsymbol{I})^{-1}\\boldsymbol{X}^T\\boldsymbol{y} \\big]\n",
    "$$\n",
    "$$\n",
    "= (\\boldsymbol{X}^T\\boldsymbol{X} + \\boldsymbol{\\lambda}\\boldsymbol{I})^{-1}\\boldsymbol{X}^T \\mathbb{E} (\\boldsymbol{y}) = \n",
    "( \\boldsymbol{X}^T\\boldsymbol{X} + \\boldsymbol{\\lambda}\\boldsymbol{I})^{-1}\\boldsymbol{X}^T \\boldsymbol{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6f914",
   "metadata": {},
   "source": [
    "**b)** Why do we say that Ridge regression gives a biased estimate? Is this a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3042c-c5f9-4eef-b026-cebd121054f3",
   "metadata": {},
   "source": [
    "We state that an estimator is unbiased if it is $\\mathbb{E}(X) = X$. Because $ \\mathbb{E} \\big[ \\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}} \\big] \\neq {\\boldsymbol{\\beta}}$, we know that Ridge regression gives a biased estimate for the parameter $\\boldsymbol{\\beta}$.\n",
    "\n",
    "This means every time we use it, it will have a tendency either to over- or under-estimate our prediction. It can be problematic if over-/under-estimation is considerably high and influences prediction accuracy at noticeable level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e721fc",
   "metadata": {},
   "source": [
    "**c)** Show that the variance is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090eb1e1",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Var}[\\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}}]=\\sigma^2[  \\mathbf{X}^{T} \\mathbf{X} + \\lambda \\mathbf{I} ]^{-1}  \\mathbf{X}^{T}\\mathbf{X} \\{ [  \\mathbf{X}^{\\top} \\mathbf{X} + \\lambda \\mathbf{I} ]^{-1}\\}^{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e8697",
   "metadata": {},
   "source": [
    "We see that if the parameter $\\lambda$ goes to infinity then the variance of the Ridge parameters $\\boldsymbol{\\beta}$ goes to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05484f98-3662-48f0-b9d8-7a8702c0623a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Var}[\\hat{\\boldsymbol{\\beta}}^{\\mathrm{Ridge}}]=\n",
    "\\mathbf{Var} \\big[ (\\boldsymbol{X}^T\\boldsymbol{X} + \\boldsymbol{\\lambda}\\boldsymbol{I})^{-1}\\boldsymbol{X}^T\\boldsymbol{y} \\big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45afab66-0e30-4c62-a4a5-1667fa507a6c",
   "metadata": {},
   "source": [
    "$$\n",
    "=  ((\\boldsymbol{X}^T\\boldsymbol{X} + \\boldsymbol{\\lambda}\\boldsymbol{I})^{-1}\\boldsymbol{X}^T) \\mathbf{Var}(\\boldsymbol{y})((\\boldsymbol{X}^T\\boldsymbol{X} + \\boldsymbol{\\lambda}\\boldsymbol{I})^{-1}\\boldsymbol{X}^T)^T\n",
    "=\\sigma^2  ((\\boldsymbol{X}^T\\boldsymbol{X} + \\boldsymbol{\\lambda}\\boldsymbol{I})^{-1}\\boldsymbol{X}^T)((\\boldsymbol{X}^T\\boldsymbol{X} + \\boldsymbol{\\lambda}\\boldsymbol{I})^{-1}\\boldsymbol{X}^T)^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\sigma^2  (\\boldsymbol{X}^T\\boldsymbol{X} + \\boldsymbol{\\lambda}\\boldsymbol{I})^{-1}\\boldsymbol{X}^T\\boldsymbol{X}((\\boldsymbol{X}^T\\boldsymbol{X} + \\boldsymbol{\\lambda}\\boldsymbol{I})^{-1})^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc300b",
   "metadata": {},
   "source": [
    "## Exercise 3: Deriving the expression for the Bias-Variance Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb86010",
   "metadata": {},
   "source": [
    "The aim of this exercise is to derive the equations for the bias-variance tradeoff to be used in project 1.\n",
    "\n",
    "The parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ are found by optimizing the mean squared error via the so-called cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a0d1d",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\beta}) =\\frac{1}{n}\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2=\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831db06c",
   "metadata": {},
   "source": [
    "**a)** Show that you can rewrite  this in terms of a term which contains the variance of the model itself (the so-called variance term), a\n",
    "term which measures the deviation from the true data and the mean value of the model (the bias term) and finally the variance of the noise Note that in order to be able to evaluate the bias them, you will need to approximate the function $f$ with the model ${\\bf y}$.\n",
    "show that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc52b3c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]=\\mathrm{Bias}[\\tilde{y}]+\\mathrm{var}[\\tilde{y}]+\\sigma^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb50416",
   "metadata": {},
   "source": [
    "with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49bdbb4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{Bias}[\\tilde{y}]=\\mathbb{E}\\left[\\left(\\boldsymbol{y}-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right]\\right)^2\\right],\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5554a",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1054343",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathrm{var}[\\tilde{y}]=\\mathbb{E}\\left[\\left(\\tilde{\\boldsymbol{y}}-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right]\\right)^2\\right]=\\frac{1}{n}\\sum_i(\\tilde{y}_i-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right])^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1451294e-f282-49cb-b680-b384bc10363b",
   "metadata": {},
   "source": [
    "Our knowledge that we use:\n",
    "\n",
    "$\\mathbb{E}(\\epsilon) = 0 $ and $\\mathrm{var}(\\epsilon) = \\sigma^2 $\n",
    "\n",
    "\n",
    "$\\boldsymbol{y} = x \\boldsymbol{\\beta} + \\epsilon = f(x) + \\epsilon$\n",
    "\n",
    "$ \\boldsymbol{\\tilde{y}} = x \\boldsymbol{\\tilde{\\beta}} = \\tilde{f}(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf9c20-a449-41df-b39d-da5e370a9aa3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]=\n",
    "\\mathbb{E}\\left[(f(x) + \\epsilon - \\tilde{f}(x))^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f102cdb5-d302-47a1-8d95-b4534d101a24",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\mathbb{E}\\left[((f(x) - \\tilde{f}(x)) + \\epsilon)^2\\right]=\n",
    "\\mathbb{E}\\left[(f(x) - \\tilde{f}(x))^2 + \\epsilon^2 - 2((f(x) - \\tilde{f}(x))\\epsilon\\right]=\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f3af8-2038-402e-8661-c539f1b36bdc",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left[(f(x) - \\tilde{f}(x))^2\\right] + \\mathbb{E}\\left[\\epsilon^2\\right] - \n",
    "\\underbrace{\\mathbb{E}\\left[2((f(x) - \\tilde{f}(x))\\epsilon\\right]}_{\\text{0}} =\n",
    "\\mathbb{E}\\left[(f(x) - \\tilde{f}(x))^2\\right] + \\mathbb{E}\\left[\\epsilon^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a338d97-04d5-4e02-88d5-866ee01d5d44",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "Since $\\epsilon$ is independent from ${\\tilde{y}}$:  COV($\\epsilon, {\\tilde{y}}$) = 0.\n",
    "Thus, COV($\\epsilon, {{y} - \\tilde{y}}$) = 0, and because they are independent, we can use the following property of expectation:\n",
    "\n",
    "\"For 2 independent variables $\\mathbb{E}(XY) = \\mathbb{E}(X)\\mathbb{E}(Y)$\"\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[2((f(x) - \\tilde{f}(x))\\epsilon\\right] =\n",
    "\\mathbb{E}\\left[2((f(x) - \\tilde{f}(x))\\right]\\mathbb{E}\\left[\\epsilon\\right] = 0\n",
    "$$\n",
    "Because as we stated above $\\mathbb{E}(\\epsilon) = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e15cca-7167-446a-8684-a1a1c32468a3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right] =\n",
    "\\mathbb{E}\\left[(f(x) - \\tilde{f}(x))^2\\right] + \\underbrace{\\mathbb{E}\\left[\\epsilon^2\\right]}_{\\text{var}[\\epsilon]}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb53c97-b974-48f7-8812-e21d41847b33",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "Also, from the condition we have:\n",
    "    \n",
    "$$\n",
    "\\mathrm{var}[\\tilde{y}]=\\mathbb{E}\\left[\\left(\\tilde{\\boldsymbol{y}}-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right]\\right)^2\\right]\n",
    "=\\frac{1}{n}\\sum_i(\\tilde{y}_i-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right])^2.\n",
    "$$\n",
    "\n",
    "We can use it to express that:\n",
    "$\\mathrm{var}[\\tilde{y}]=\\mathbb{E}\\left[\\left(\\epsilon-\\mathbb{E}\\left[\\boldsymbol\\epsilon\\right]\\right)^2\\right]$ and since we have $\\mathbb{E}\\left[\\boldsymbol\\epsilon\\right] = 0$\n",
    "\n",
    "$$\n",
    "\\mathrm{var}[\\epsilon]=\\mathbb{E}\\left[(\\epsilon)^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda5656d-8c54-469b-bb8d-bb56dbad7d03",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left[(f(x) - \\tilde{f}(x))^2\\right]\n",
    "\\overset{+/-\\mathbb{E}(\\tilde{f}(x))}{=} \n",
    "\\mathbb{E}\\left[(f(x) + \\mathbb{E}(\\tilde{f}(x)) - \\mathbb{E}(\\tilde{f}(x)) - \\tilde{f}(x))^2\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0eb62-d1cf-4622-b5dc-ec286dfe0eee",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left[ \\left( f(x) + \\mathbb{E}(\\tilde{f}(x))\\right)^2 + \\left( \\mathbb{E}(\\tilde{f}(x)) - \\tilde{f}(x) \\right)^2 \n",
    "- 2 \\left( ( f(x) + \\mathbb{E}(\\tilde{f}(x))) (\\mathbb{E}(\\tilde{f}(x)) - \\tilde{f}(x)) \\right) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dbbb90-742c-4105-92f6-1e6a7e5622ed",
   "metadata": {},
   "source": [
    "$$\n",
    "\\underbrace{\\mathbb{E} \\left( f(x) + \\mathbb{E}(\\tilde{f}(x)) \\right)^2}_{\\text{Bias}} \n",
    "+ \\underbrace{\\mathbb{E} \\left( \\mathbb{E}(\\tilde{f}(x)) - \\tilde{f}(x) \\right)^2}_{\\text{var}} \n",
    "- 2 \\mathbb{E} \\left( (f(x) + \\mathbb{E}(\\tilde{f}(x))) \\cdot \\underbrace{(\\mathbb{E}(\\tilde{f}(x)) - \\tilde{f}(x))}_{\\text{will become 0}} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fbfcd7",
   "metadata": {},
   "source": [
    "**b)** Explain what the terms mean and discuss their interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a7dda-f3f2-4243-ad2f-54747f9572b9",
   "metadata": {},
   "source": [
    "If we get all our terms together we will get:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986f80a-4f2b-4b2c-aa11-7569ce9d406d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]=\n",
    "\\mathbb{E}\\left[(f(x) - \\tilde{f}(x))^2\\right] + \\underbrace{\\mathbb{E}\\left[\\epsilon^2\\right]}_{\\text{var}[\\epsilon]} = \n",
    "\\underbrace{\\mathbb{E} \\left( f(x) + \\mathbb{E}(\\tilde{f}(x)) \\right)^2}_{\\text{Bias}} \n",
    "+ \\underbrace{\\mathbb{E} \\left( \\mathbb{E}(\\tilde{f}(x)) - \\tilde{f}(x) \\right)^2}_{\\text{var}} + \\underbrace{\\mathbb{E}\\left[\\epsilon^2\\right]}_{\\text{var}[\\epsilon]} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c5868-4326-4940-aecd-9f6d0d950e0d",
   "metadata": {},
   "source": [
    "Bias and Variance correspond to $\\mathrm{Bias}[\\tilde{y}]$ and $\\mathrm{Var}[\\tilde{y}]$ as I have trouble setting such a complex expression in LaTeX format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09580d6c-3130-4798-a371-26e37a0f0567",
   "metadata": {},
   "source": [
    "- $\\mathrm{var}[\\epsilon]$ is the variance of the error, which we can not change\n",
    "\n",
    "- $\\mathrm{Bias}[\\tilde{y}]$ is a bias of our prediction, meaning the magnitude we will under- or over-estimate.\n",
    "\n",
    "- $\\mathrm{Var}[\\tilde{y}]$ is a variance, meaning how largely our prediction will vary.\n",
    "\n",
    "We can not affect $\\mathrm{var}[\\epsilon]$, but we can choose parameters in our model to balance $\\mathrm{Bias}[\\tilde{y}]$ and $\\mathrm{Var}[\\tilde{y}]$ to get an optimal result. \n",
    "\n",
    "We might allow some bias to get a smaller variance. It is also known as the \"variance-bias tradeoff\" where we trying to balance bias and variance to get an optimal model with smalles error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8b9d1",
   "metadata": {},
   "source": [
    "## Exercise 4: Computing the Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e012430",
   "metadata": {},
   "source": [
    "Before you compute the bias and variance of a real model for different complexities, let's for now assume that you have sampled predictions and targets for a single model complexity using bootstrap resampling.\n",
    "\n",
    "**a)** Using the expression above, compute the mean squared error, bias and variance of the given data. Check that the sum of the bias and variance correctly gives (approximately) the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5bf581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 100\n",
    "bootstraps = 1000\n",
    "\n",
    "predictions = np.random.rand(bootstraps, n) * 10 + 10\n",
    "targets = np.random.rand(bootstraps, n)\n",
    "\n",
    "mse = ...\n",
    "bias = ...\n",
    "variance = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1dc621",
   "metadata": {},
   "source": [
    "**b)** Change the prediction values in some way to increase the bias while decreasing the variance.\n",
    "\n",
    "**c)** Change the prediction values in some way to increase the variance while decreasing the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da63362",
   "metadata": {},
   "source": [
    "**d)** Perform a bias-variance analysis of a polynomial OLS model fit to a one-dimensional function by computing and plotting the bias and variances values as a function of the polynomial degree of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd5855e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures # use the fit_transform method of the created object!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "bootstraps = 1000\n",
    "\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1)\n",
    "\n",
    "biases = []\n",
    "variances = []\n",
    "mses = []\n",
    "\n",
    "#for p in range(1, 5):\n",
    "#    predictions = ...\n",
    "#    targets = ...\n",
    "#    for b in range(bootstraps):\n",
    "#        x_sample, y_sample = ...\n",
    "#        X = ...\n",
    "#        X_train, X_test, y_train, y_test = ...\n",
    "#\n",
    "#        predictions[b, :] = \n",
    "#        targets[b, :] = \n",
    "#        \n",
    "#    biases.append(...)\n",
    "#    variances.append(...)\n",
    "#    mses.append(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b8461",
   "metadata": {},
   "source": [
    "**e)** Discuss the bias-variance trade-off as function of your model complexity (the degree of the polynomial).\n",
    "\n",
    "**f)** Compute and discuss the bias and variance as function of the number of data points (choose a suitable polynomial degree to show something interesiting)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
