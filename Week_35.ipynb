{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49407aa4",
   "metadata": {},
   "source": [
    "# Exercise 1 - Finding the Derivative of Matrix-Vector Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008ddc5-674f-441c-8785-f89a3442cec0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "a) Consider the expression\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\mathbf{x}}(\\mathbf{a^T}\\mathbf{x})\n",
    "$$\n",
    "\n",
    "\n",
    "Where a and x are column-vectors with length n.\n",
    "\n",
    "What is the shape of the expression we are taking the derivative of?\n",
    "\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\mathbf{a^T} \\cdot \\mathbf{x} = \\begin{pmatrix} a_0 & a_1 & ... & a_{n-1} \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ ...\\\\ x_{n-1} \\end{pmatrix} = \\begin{pmatrix} a_0 * x_0 + a_1 * x_1  + ...+  a_{n-1} * x_{n-1} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- So the shape will be (1,1)\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "What is the shape of the thing we are taking the derivative with respect to?\n",
    "</div>\n",
    "\n",
    "- as x is a column-vectors with length n, we are taking derivative with respect to values from $x_0$ to $x_{n-1}$, so the shape will be (1,1)\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "What is the shape of the result of the expression?\n",
    "</div>\n",
    "\n",
    "- When we derive the following expression $\\begin{pmatrix} a_0 * x_0 + a_1 * x_1  + ...+  a_{n-1} * x_{n-1} \\end{pmatrix}$\n",
    " with respect to the vector $\\mathbf{x}$ we will get a column-vectors of length n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb18d3-a7d0-4b91-bc16-5c6021835f42",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">\n",
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "b) Show that\n",
    "\n",
    "Demonstrate that \n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial \\boldsymbol{x}} = \\boldsymbol{a^T},\n",
    "$$\n",
    "</div>\n",
    "\n",
    "Once we derive with respect to $x_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial \\boldsymbol{x}} =  \\begin{pmatrix} \n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial x_0} \\\\ \n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial x_1} \\\\ \n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{x})}{\\partial x_{n-1}}\n",
    "\\end{pmatrix} = \\begin{pmatrix} \n",
    "\\frac{\\partial (a_0 * x_0 + a_1 * x_1  + ...+  a_{n-1} * x_{n-1})}{\\partial x_0} \\\\ \n",
    "\\frac{\\partial (a_0 * x_0 + a_1 * x_1  + ...+  a_{n-1} * x_{n-1})}{\\partial x_1} \\\\ \n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial (a_0 * x_0 + a_1 * x_1  + ...+  a_{n-1} * x_{n-1})}{\\partial x_{n-1}}\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix} a_0 \\\\ a_1 \\\\ ...\\\\ a_{n-1} \\end{pmatrix}\n",
    "=\\boldsymbol{a}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0365eb-eb12-40e2-962b-509aede16421",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">\n",
    "\n",
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "c) Show that\n",
    "\n",
    "Demonstrate that \n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T\\boldsymbol{A}\\boldsymbol{a})}{\\partial \\boldsymbol{a}} = \\boldsymbol{a}^T(\\boldsymbol{A}+\\boldsymbol{A}^T),\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ca476-dc9e-4d66-95eb-9038e5c49a6d",
   "metadata": {},
   "source": [
    "We know that \n",
    "\n",
    "$$ \\frac{d}{d\\mathbf{x}}(\\mathbf{a}\\mathbf{x}) = \\mathbf{a^T} \\; \\; \\;\n",
    "\\frac{d}{d\\mathbf{x}}(\\mathbf{a^T}\\mathbf{x}) = \\mathbf{a}$$\n",
    "\n",
    "With use of the derivative of a product rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial(u(x)v(x))}{\\partial \\boldsymbol{x}} = u'(x)v(x) + u(x)v'(x)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e086f2-eaac-4ec0-b8a6-377a47f651dd",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial (\\boldsymbol{a}^T \\boldsymbol{A} \\boldsymbol{a})}{\\partial \\boldsymbol{a}} =\n",
    "(\\boldsymbol{a}^T)' \\boldsymbol{A} \\mathbf{a} + \\boldsymbol{a}^T \\boldsymbol{A} \\boldsymbol{a}' = \n",
    "\\boldsymbol{A} \\mathbf{a} + (\\boldsymbol{a}^T \\boldsymbol{A})^T =\n",
    "\\boldsymbol{A} \\mathbf{a} + \\boldsymbol{A}^T \\boldsymbol{a} =\n",
    " (\\boldsymbol{A} + \\boldsymbol{A}^T) \\boldsymbol{a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d3f3bf-c1b8-4c6f-b0c6-6f85dc4b5d97",
   "metadata": {},
   "source": [
    "https://www.uio.no/studier/emner/matnat/math/STK2100/v25/undervisningsmateriell/presentasjoner/stk2100-ch3.pdf\n",
    "#\n",
    "# slide 5 ask!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67badcf",
   "metadata": {},
   "source": [
    "# Exercise 2 - Deriving the expression for OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d35359-84d9-42b3-b4b9-c4e73fb24e2a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "Exercise 2 - Deriving the expression for OLS\n",
    "The ordinary least squares method finds the parameters $\\boldsymbol{\\theta}$ which minimizes the squared error between our model $\\boldsymbol{X\\theta}$ and the true values $\\boldsymbol{y}$.\n",
    "\n",
    "To find the parameters $\\boldsymbol{\\theta}$ which minimizes this error, we take the derivative of the squared error expression with respect to $\\boldsymbol{\\theta}$, and set it equal to 0.\n",
    "\n",
    "a) Very briefly explain why the approach above finds the parameters which minimizes this error.\n",
    "</div>\n",
    "\n",
    "we use sqared error to ensure continuiry of the function, and want to find equation that gives minimal difference between true and predicted values. To find function's extremum, we derive the function with respect to the parameter we are looking for and set the equation to be equal to 0. After that, we rearrange the equation to find the estimator of the parameter of our interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab4c387-bfb5-4fcc-b606-1ea8f86c36f1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "We typically write the squared error as\n",
    "\n",
    "$$\n",
    "\\vert\\vert\\boldsymbol{y} - \\boldsymbol{X\\theta}\\vert\\vert^2\n",
    "$$\n",
    "\n",
    "which we can rewrite in matrix-vector form as\n",
    "\n",
    "$$\n",
    "\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\theta}\\right)\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac66c2-8d26-42e4-8f46-15ec85f2cbdd",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">\n",
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "b) If $\\boldsymbol{X}$ is invertible, what is the expression for the optimal parameters $\\boldsymbol{\\theta}$? (**Hint:** Don't compute any derivatives, but solve $\\boldsymbol{X\\theta}=\\boldsymbol{y}$ for $\\boldsymbol{\\theta}$)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae15a46",
   "metadata": {},
   "source": [
    "If $\\boldsymbol{X\\theta}=\\boldsymbol{y}$ is consistent (in other words we can find at leat one solution to that eqation $\\boldsymbol{X\\theta}=\\boldsymbol{y}$) and $\\boldsymbol{X}$ is invertible, than:\n",
    "\n",
    "$$\\boldsymbol{\\theta}=\\boldsymbol{x}^{-1}\\boldsymbol{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aba14e-4cec-412f-b374-4e89b71227a7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "c) Show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)}{\\partial \\boldsymbol{s}} = -2\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\boldsymbol{A}\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f0227c-5161-48ce-8b16-50911657fc75",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)^T\\left(\\boldsymbol{x}-\\boldsymbol{A}\\boldsymbol{s}\\right)}{\\partial \\boldsymbol{s}} =\n",
    "\\frac{\\partial \\left(\\boldsymbol{x}^T\\boldsymbol{x} -  \\boldsymbol{x}^T \\boldsymbol{A}\\boldsymbol{s} - (\\boldsymbol{A}\\boldsymbol{s})^T\\boldsymbol{x} + (\\boldsymbol{A}\\boldsymbol{s})^T \\boldsymbol{A}\\boldsymbol{s} \\right)}{\\partial \\boldsymbol{s}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{A}\\boldsymbol{s})^T\\boldsymbol{x} = \n",
    "\\boldsymbol{s}^T\\boldsymbol{A}^T\\boldsymbol{x} = \n",
    "\\boldsymbol{x}^T\\boldsymbol{A}\\boldsymbol{s}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1898b8e-05ed-4062-98f3-613059fb4c5b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\left(\\boldsymbol{x}^T\\boldsymbol{x} -  2\\boldsymbol{x}^T \\boldsymbol{A}\\boldsymbol{s} + \\boldsymbol{s}^T\\boldsymbol{A}^T \\boldsymbol{A}\\boldsymbol{s} \\right)}{\\partial \\boldsymbol{s}}\n",
    "= -2\\boldsymbol{A}^T\\boldsymbol{x} + (\\boldsymbol{A}^T\\boldsymbol{A} + (\\boldsymbol{A}^T\\boldsymbol{A})^T)\\boldsymbol{s}\n",
    "= -2\\boldsymbol{A}^T\\boldsymbol{x} + 2\\boldsymbol{A}^T\\boldsymbol{A}\\boldsymbol{s}\n",
    "= -2\\boldsymbol{A}^T(\\boldsymbol{x} - \\boldsymbol{A}\\boldsymbol{s})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268bbdb-1569-47fd-9ec2-9a005593ed84",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "d)Using the expression from **c)**, but substituting back in $\\boldsymbol{\\theta}$, $\\boldsymbol{y}$ and $\\boldsymbol{X}$, find the expression for the optimal parameters $\\boldsymbol{\\theta}$ in the case that $\\boldsymbol{X}$ is not invertible, but $\\boldsymbol{X^T X}$ is, which is most often the case.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\theta}_{OLS}} = ...\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e0f26",
   "metadata": {},
   "source": [
    "$\\boldsymbol{s} = \\boldsymbol{\\theta}$; $\\boldsymbol{x} = \\boldsymbol{y}$; $\\boldsymbol{A} = \\boldsymbol{X}$\n",
    "\n",
    "to find the optimal parameter we need to set expression from part c) equal to 0:\n",
    "\n",
    "$$\n",
    "-2\\boldsymbol{X}^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\hat{\\theta}}) = 0\n",
    "$$\n",
    "We can cansell -2 by multiplying both sides to -1/2 and with rearrangement will get:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\hat{\\theta}} = \\boldsymbol{X}^T\\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\hat{\\theta}} = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b221665",
   "metadata": {},
   "source": [
    "# Exercise 3 - Creating feature matrix and implementing OLS using the analytical expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c741e29d-9765-4f7a-be0d-75ff45eb94f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48117e57-aa4e-4993-a479-e4225b305789",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "income = np.array([116., 161., 167., 118., 172., 163., 179., 173., 162., 116., 101., 176., 178., 172., 143., 135., 160., 101., 149., 125.])\n",
    "children = np.array([5, 3, 0, 4, 5, 3, 0, 4, 4, 3, 3, 5, 1, 0, 2, 3, 2, 1, 5, 4])\n",
    "spending = np.array([152., 141., 102., 136., 161., 129.,  99., 159., 160., 107.,  98., 164., 121.,  93., 112., 127., 117.,  69., 156., 131.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e68210",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "a) Create a feature matrix for the features income and children, including an intercept column of ones at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d8bcc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((n, 3))\n",
    "X[:, 0] = 1\n",
    "X[:, 1] = income\n",
    "X[:, 2] = children\n",
    "\n",
    "y = spending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f3b7a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "b) Use the expression from 3d) to find the optimal parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ for predicting spending based on these features. Create a function for this operation, as you are going to need to use it a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09a3ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLS_parameters(X, y):\n",
    "    xT = X.T # transpose matrix x\n",
    "    xTx_inv = inv(np.matmul(xT, X)) \n",
    "    xTy = np.matmul(xT, y)\n",
    "    return np.matmul(xTx_inv, xTy)\n",
    "\n",
    "beta = OLS_parameters(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffbc9fb",
   "metadata": {},
   "source": [
    "# Exercise 4 - Fitting a polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "382baee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f11fc3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "\n",
    "a) Create a feature matrix $\\boldsymbol{X}$ for the features $ x, x^2, x^3, ..., x^p$, including an intercept column of ones at the start. Make this into a function, as you will do this a lot over the next weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4faa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, p):\n",
    "    n = len(x)\n",
    "    X = np.zeros((n, p + 1))\n",
    "    X[:, 0] = 1\n",
    "    for i in (1, p):\n",
    "        print(i)\n",
    "    #X[:, 1] = ...\n",
    "    #X[:, 2] = ...\n",
    "    # could this be a loop?\n",
    "\n",
    "X = polynomial_features(x, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6308b39",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f0f0; border: 1px solid #d3d3d3; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "b) Use the expression from 3d) to find the optimal parameters $\\boldsymbol{\\hat{\\beta}_{OLS}}$ for predicting \n",
    " $\\boldsymbol{y}$ based on these features. If you have done everything right so far, this code will not need changing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "034f502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta = OLS_parameters(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d703f788",
   "metadata": {},
   "source": [
    "**c)** Like in exercise 4 last week, split your feature matrix and target data into a training split and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29171358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3509f",
   "metadata": {},
   "source": [
    "**d)** Train your model on the training data(find the parameters which best fit) and compute the MSE on both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e346f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e431889",
   "metadata": {},
   "source": [
    "**e)** Do the same for each polynomial degree from 2 to 10, and plot the MSE on both the training and test data as a function of polynomial degree. The aim is to reproduce Figure 2.11 of [Hastie et al](https://github.com/CompPhysics/MLErasmus/blob/master/doc/Textbooks/elementsstat.pdf). Feel free to read the discussions leading to figure 2.11 of Hastie et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceb57457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b5954",
   "metadata": {},
   "source": [
    "**f)** Interpret the graph. Why do the lines move as they do? What does it tell us about model performance and generalizability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2acfb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5994f0c5",
   "metadata": {},
   "source": [
    "## Exercise 5 - Comparing your code with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f595b7a",
   "metadata": {},
   "source": [
    "When implementing different algorithms for the first time, it can be helpful to double check your results with established implementations before you go on to add more complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab310c1",
   "metadata": {},
   "source": [
    "**a)** Make sure your `polynomial_features` function creates the same feature matrix as sklearns PolynomialFeatures.\n",
    "\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b964d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73c32c52",
   "metadata": {},
   "source": [
    "**b)** Make sure your `OLS_parameters` function computes the same parameters as sklearns LinearRegression with fit_intercept set to False, since the intercept is included in the feature matrix. Use `your_model_object.coef_` to extract the computed parameters.\n",
    "\n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b04126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
